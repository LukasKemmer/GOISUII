\documentclass[ngerman, a4paper,12pt]{article}

%\usepackage[applemac]{inputenc} % Bei Benutzung von Apple-Betriebssystemen bitte durch ``\usepackage[applemac]{inputenc}'' ersetzen.
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}

\usepackage[ngerman]{babel} 
\usepackage{fixltx2e}
\usepackage{tabularx}
\usepackage{booktabs}
\usepackage{placeins}
\usepackage{eurosym}
\usepackage{amssymb,amsmath}
\usepackage{mathtools}
\usepackage{graphicx} 
\usepackage{color}
\definecolor{kit}{cmyk}{1,0,0.6,0}

\usepackage{hyperref}
\hypersetup{pdftoolbar=true,
            pdfmenubar=true,
            pdfpagemode=UseOutlines,
            bookmarksnumbered=true,
            linktocpage=true,
            colorlinks=false,
            %backref, % Entkommentieren, um zu sehen, ob alle Literaturstellen im Text zitiert werden.
            colorlinks=false
            }

\newtheorem{definition}{Definition}[section]
\newtheorem{satz}[definition]{Satz}
\newtheorem{lemma}[definition]{Lemma}
\newtheorem{korollar}[definition]{Korollar}
\newtheorem{proposition}[definition]{Proposition}
\newtheorem{bemerkung}[definition]{Bemerkung}
\newtheorem{beispiel}[definition]{Beispiel}
\newtheorem{problem}[definition]{Problem}
\newtheorem{Voraussetzung}[definition]{Voraussetzung}
\newtheorem{algorithmus}[definition]{Algorithmus}
\newtheorem{vermutung}[definition]{Vermutung}

\setlength{\parindent}{0pt}
\parskip1.5ex

\newcommand{\R}{\mathbb R} % Beispiel für die Definition eines eigenen Befehls

\begin{document}

\begin{flushleft}
\vspace*{-100pt}
\textbf{Institut f\"ur Operations Research \\
Prof. Dr. Oliver Stein \\}
Sommersemester 2018
\vspace*{15pt}
\end{flushleft}

\begin{flushright}
\vspace*{-80pt}
\includegraphics[scale=0.5]{kit_logo}
\vspace*{15pt}
\end{flushright}

\begin{center}
\textbf{Zweite Bonusübung zur Vorlesung \\
\emph{Globale Optimierung I}}        
\end{center}

\begin{table}[h]
	\centering
	\begin{tabularx}{\textwidth}{X X X X X}
		 & Vorname & Nachname & Matr.Nr. & Bachelor / Master \\
		\toprule
		1. Mitglied & Leon & Qadirie & 1720201 &  Master\\
		2. Mitglied & Lukas & Kemmer & 1725171 &  Master\\
		\bottomrule
	\end{tabularx}
\end{table}
\textbf{Aufgabe S2.1} \\
(a) Das lineare Gleichungssystem
\begin{equation}
  (A^TA+\alpha R^TR)x=A^Tb
\end{equation}
besitzt genau dann eine eindeutige Lösung, wenn $A^TA+\alpha R^TR$ invertierbar ist. Weiterhin gilt
\begin{equation*}
	\begin{split}
		Kern(A^TA + \alpha R^T R) &= \{x \in \mathbb{R}^n \ | \ A^TAx + \alpha R^TRx = 0 \} \\
	&=\{x \in \mathbb{R}^n \ | \ x^TA^TAx + \alpha x^TR^TRx = 0 \} \\
	\end{split}
\end{equation*}
mit $x^TA^TAx = \| Ax \| _2^2 \geq 0$ und $\alpha x^TR^TRx = \alpha \| Rx \| \geq 0$ (wegen $\alpha >0$). Damit folgt, dass 
\begin{equation}
	x^TA^TAx + \alpha x^TR^TRx = 0
\end{equation}
nur dann erfüllt ist, wenn beide Summanden $0$ sind, d.h. wir suchen ein $x$ für das sowohl $x^TA^TAx=0$ als auch $x^TR^TRx=0$ gilt (wegen $\alpha>0$ muss $\alpha$ hier nicht berücksichtigt werden). Aus
\begin{equation*}
	\begin{split}
		Kern(A) \cap Kern(B) &= \{x \in \mathbb{R}^n \ | \ Ax=0 \} \cap \{x \in \mathbb{R}^n \ | \ Rx=0 \} \\
		&=\{x \in \mathbb{R}^n \ | \ x^TA^TAx=0 \land x^TR^TRX=0 \} \\
		&=\{0\}
	\end{split}
\end{equation*}
folgt, dass diese Vorderung nur für $x=0$ erfüllt ist. Damit gilt $Kern(A^TA + \alpha R^TR) = \{0\}$ und damit nach Satz 5 der mathematischen Grundlagen, dass $A^TA + \alpha R^TR$ invertierbar ist. Die eindeutige Lösung des linearen Gleichungssystems ist dann
\begin{equation*}
	\begin{split}
	(A^TA+\alpha R^TR)x &=A^Tb \\
	\Leftrightarrow x&= (A^TA+\alpha R^TR)^{-1} A^Tb	
	\end{split}
\end{equation*}
% und es folgt
%\begin{equation}
%	x = (A^TA+\alpha R^TR)^-1 A^Tb.
%\end{equation}
%Gemäß $A6$ besitzt $A^TA+\alpha R^TR$ vollen Rang, so $Kern(A^TA+\alpha R^TR=\{0\}$
%Kern(A)\cap Kern(R)=\{0\} \Rightarrow \{x\in \mathbb R^n|Ax=0\}\cap\{x\in\mathbb R^n|Rx=0\}
%Es gilt: $Kern(A^TA+\alpha R^TRx)$\\[10pt]
%=$\{x\in\mathbb R^n|(A^TA+\alpha R^TR)x=0\}$\\[10pt]
%=$\{x\in \mathbb R^n|\underbrace{A^TA}_{n\times n,\succeq 0}x\}+\alpha\underbrace{R^TR}_{n\times n,\succeq 0}x=0\}=\{0\}\quad$ \footnote{Positiv semidefinit da Gram-Matrix}\\[10pt]
%=$\{x\in \mathbb R^n|x^T\underbrace{A^TA}_{\geq 0}x\}+\alpha x^T\underbrace{R^TR}_{\geq 0}x=0\}=\{0\}$\\[10pt]
%da nur für $x=0:(Ax=0)\wedge (Rx=0)$.
%\textcolor{red}{Damit ist $A^TA+\alpha R^TR$ invertierbar}
\par
(b) Sei $f: \mathbb{R}^n \rightarrow \mathbb{R}$ mit $f(x) = \|Ax-b \|_2^2 + \alpha \|Rx \|_2^2$. Wegen $D^2( \|x \|_2^2)=2I \succeq 0$, der Linearität von $Ax-b$ sowie $Rx$ und $\alpha > 0$ folgt mit Übung 4.6 und Satz 2.5.3 die Konvexität von $f$ als Summe konvexer Funktionen (merke, dass es sich um ein unrestringiertes Problem mit $M=\mathbb{R}^n$ handelt und der $\mathbb{R}^n$ eine konvexe Menge ist). Mit Satz 2.4.5 und Korollar 2.4.6 folgt, dass die kritischen Punkte von $f$ genau die globalen Minimalpunkte sind. Es gilt
\begin{equation}
	\nabla f(x) = 2A^T(Ax-b) + 2\alpha R^TRx
\end{equation}
und damit
\begin{equation*}
	\begin{split}
		2A^T(Ax-b) + 2\alpha R^TRx &= 0 \\
		\Leftrightarrow A^TAx-A^Tb+\alpha R^TRx &=0 \\
		\Leftrightarrow (A^TA+\alpha R^TR)x &=A^Tb.
	\end{split}
\end{equation*}
Damit ist die eindeutige Lösung des linearen Gleichungssystems aus (a) genau der eindeutige globale Minimalpunkt von $P$.
%Es handelt sich bei dem Gleichungssystem aus (a) offensichtlich um ein konvexes Optimierungsproblem.
%Entsprechend sind alle kritischen Punkte auch Nullstellen.
%Es gilt:
%$\nabla_x f(x)=A^T(Ax-b)+\alpha R^TRx\stackrel{!}{=}0$\\[5pt]
%$\Leftrightarrow A^TAx-A^Tb+\alpha R^TRx=0$\\[5pt]
%$\Leftrightarrow (A^TA+\alpha R^TR)x=A^Tb$\\[5pt]
%q.e.d.
\par
\textbf{Aufgabe S2.2} \\
(a) Das lineare Gleichungssystem aus S2.1 (a) kann gelöst werden mit $R \coloneqq L, \ b \coloneqq y, \ A \coloneqq I$ mit
%R:=L, \textcolor{red}{b:=Y}, A:=I.
\begin{equation*}
	L = \begin{pmatrix}
	1 &-1 & 0 \cdots &0 &0\\
	0 & 1 & -1 \cdots &0 &0\\
	\vdots & \vdots & \vdots & \vdots & \vdots \\
	0 & 0 & \cdots & 1 & -1\\ 
	\end{pmatrix} \in \mathbb R^{(n-1)\times n}.
\end{equation*}
Der Rest der Lösung ist in dem angehängten Python-Code dokumentiert.
\par
(b) %\textcolor{red}{Why not? Deutlich smoother?!. Ergibt Sinn, so Funktion smooth ist. Etc.; Ausgelassen da unklar}
Mithilfe von $RL_2$ erhalten wir Approximationen der Werte $f(t_k)$ die augenscheinlich einer glatten Funktion entstammen. Bei Betrachtung der zugrunde liegenden Daten können jedoch drei potentielle Sprungstellen identifiziert werden, bei denen die Sprünge deutlich stärker ausfallen als das sonst vorliegende Noise. Es ist daher fraglich, ob es sich bei dem Ergebniss von $RL_2$ um eine gute Approximation handelt. Darüber hinaus sollte beachtet werden, dass das Ergebnis von $RL_2$ maßgeblich von der Wahl des Parameters $\alpha$ abhängt. Da jedoch nicht klar ist, welches $\alpha$ zu den besten Approximationen von $f8t_k)$ führt muss das Ergebniss von $RL_2$ auch hier mit Vorsicht genossen werden.
\par
(c) Es gilt
\begin{equation*}
	\begin{split}
	CRL_1: \min_{x\in\mathbb R^n,z}\|x-y\|_2^2+\alpha\|z\|_1 \quad s.t. \quad Lx = z \Leftrightarrow Lx - z=0
	\end{split}
\end{equation*}
Für die Lagrange-funktion folgt damit
\begin{equation}
\label{eq:lagrange}
L(x,z,\mu)=\|x-y\|_2^2+\alpha \|z\|_1+\mu^T(Lx - z).
\end{equation}
Mithilfe von \refeq{eq:lagrange} ergibt sich das Lagrange-Dualproblem
\begin{equation*}
	\begin{split}
	LD: &\max_{\mu} \inf_{x,z} \|x-y\|_2^2+\alpha\|z\|_1 - \mu^Tz + \mu^TLx \\
			&\max_{\mu} \left( \inf_{x} \left( \underbrace{\|x-y\|_2^2 + \mu^TLx}_{\eqqcolon g(x)}  \right) + \inf_{z} \left( 
			\underbrace{\alpha\|z\|_1 - \mu^Tz}_{\eqqcolon h(x)}  \right) \right).
	\end{split}
\end{equation*}
Dabei sei angemerkt, dass die Separierbarkeit des infimums analog zu Übung 1.3.2 aus dem Buch erfolgt. Wir betrachten nun zunächst die beiden Funktionen .
%\begin{equation*}
%  \Rightarrow \underbrace{\inf_{x,z} (\|x-y\|_2^2-\mu^TLx)}_{a} + \underbrace{\inf_z(\mu^Tz+\alpha\|z\|_1)}_{b}
%\end{equation*}
Für $g(x)$ gilt
\begin{equation*}
	\begin{split}
		\nabla g(x) &= 2(x-y) + L^T\mu. \\
		D^2 g(x) &= 2 I \succ 0.
	\end{split}
\end{equation*}
Damit folgt nach Satz 2.5.10, dass $g(x)$ gleichmäßig konvex ist (beachte, dass alle Eigenwerte der Hessematrix $2$ und damit echt größer $0$ sind, da bei einer Diagonalmatrix die Eigenwerte den Diagonaleinträgen der Matrix entsprechen). Mithilfe von Satz 2.4.5 folgt, dass mithilfe der kritischen Punkte die Globalen Minimalpunkte und damit das Infimum von $g(x)$ gefunden werden können. Es folgt
\begin{equation*}
	\begin{split}
	  2( \bar{x}-y)+L^T\mu &\stackrel{!}{=}0 \\
	  \Leftrightarrow \bar{x}&=y+\frac{1}{2} L^T\mu.
	\end{split}
\end{equation*}
Damit folgt 
\begin{equation*}
	\begin{split}
		\inf_x g(x) = g( \bar{x} ) &=\frac{1}{4}\mu^TLL^T\mu+\mu^TLy-\frac{1}{2}\mu^TLL^T\mu \\ &=-\frac{1}{4}\mu^TLL^T\mu+\mu^TLy.
	\end{split}
\end{equation*}
Für $h(x)$ gilt
\begin{equation*}
	\begin{split}
		\inf_x h(x)&=\inf_z \sum_{i=1}^{n-1}( -\mu_iz_i+\alpha|z_i|) \\
		  &= \sum_{i=1}^{n-1} \inf_{z_i} (-\mu_iz_i+\alpha|z_i|) \\
		  &= \sum_{i=1}^{n-1} \gamma (z_i)
		  \end{split}
\end{equation*}
mit 
%Sei\footnote{\textcolor{red}{Im Folgenden nicht alle Nebenrechnungen aufgeführt}} nun $\mu\leq\alpha$.
\begin{equation*}
\gamma(z_i) \coloneqq \begin{cases}
	0,& |\mu_i|\leq \alpha,\\
	-\infty,& |\mu_i|>\alpha.\\
\end{cases}
\end{equation*}
%wegen $z_i\geq0:\mu_iz_i+\alpha z_i=(\mu_i+\alpha)z_i$.
Insgesamt folgt 
\begin{equation*}
	\begin{split}
		\inf_z h(x) &=  \mu^Tz+\alpha\|z\|_1 \\
		&=\begin{cases}
		-\infty,&\exists i \in \{1,..., n\}:|\mu_i | >\alpha,\\
		0,&\|\mu\|_\infty\leq\alpha.\\
		\end{cases}
	\end{split}
\end{equation*}
Da $\inf_z h(x)$ für $\| \mu \|_{\infty} > \alpha $ auf $-\infty$ und damit nicht mehr in die reelen Zahlen abbildet fügen wir $\| \mu \|_{\infty}$ als Nebenbedingung zu $LD$ hinzu. Es ergibt sich
%\begin{equation*}
 % \Rightarrow \max_\mu \|x-y\|_2^2+\alpha\|z\|_1+\mu^T(z-Lx)
%\end{equation*}
\begin{equation*}
  LD:  \max_{\mu \in \mathbb{R}^{n-1}} -\frac{1}{4}\mu^TLL^T\mu + \mu^TLy,\quad \text{s.t. } \quad \|\mu\|_\infty\leq\alpha.
\end{equation*}
(d) \par



\textbf{Aufgabe S2.3} \\

(a) Es gilt $\nabla h(x) = Ax + b$. Damit kann das LP geschrieben werden als
\begin{equation*}
	LP: \quad \min_{x \in \mathbb{R}^n} \hat{x}^T Ax + b^Tx - \hat{x}^TA \hat{x} - b^T \hat{x} \quad \text{s.t.} \quad \| x \|_{\infty} \leq \alpha.
\end{equation*}
Da Konstante Terme in der Zielfunktion keinen Einfluss auf die Wahl der optimalen Punkte haben (vgl. Buch, Übung 1.3.1) ist dies equivalent zu
\begin{equation*}
	LP': \quad \min_{x \in \mathbb{R}^n} \left( \hat{x}^T A + b^T \right) x\quad \text{s.t.} \quad \| x \|_{\infty} \leq \alpha
\end{equation*}
mit $\hat{x}^T A + b^T$ einem Vektor der DImension $1xn$. Wir setzen nun $c \coloneqq A \hat{x} + b \in \mathbb{R}^n$. Damit können wir $LP'$ schreiben als
\begin{equation*}
LP': \quad \min_{x \in \mathbb{R}^n} c^T x\quad \text{s.t.} \quad \| x \|_{\infty} \leq \alpha.
\end{equation*}
Es gilt für alle $x \in \mathbb{R}^n$
\begin{equation*}
	\begin{split}
		c^Tx = \sum_{i=1}^{n} c_ix_i &\geq - \sum_{i=1}^{n} |c_i| |x_i| \\
		&\geq -\sum_{i=1}^{n} |c_i| \alpha \\
		&= -\alpha \sum_{i=1}^{n} c_i sgn(c_i) 
	\end{split}
\end{equation*}
mit $sgn(x_i)$ dem Vorzeichen von $x:i$. Es sei angemerkt, dass die zweite Ungleichung aus der Nebenbedingung $\| x \|_{\infty} \geq \alpha$ sowie $\alpha > 0$ folgt. Insgesamt gilt, dass $x^*$ mit $x^*_i = - \alpha sgn(c_i)$ nach Definition 1.1.3 ein globaler Minimalpunkt von $LP'$ und damit auch von $LP$ ist.
\par
(b)  
\par
(c) 
\par
(d) 
\par
(e)
\par
\end{document}